# Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors

Oran Gafni Adam Polyak Oron Ashual Shelly Sheynin Devi Parikh Yaniv Taigman

Meta AI Research

{oran,adampolyak,oron,shellysheynin,dparikh,yaniv}@fb.com

###### Abstract

Recent text-to-image generation methods provide a simple yet exciting conversion capability between text and image domains. While these methods have incrementally improved the generated image fidelity and text relevancy, several pivotal gaps remain unanswered, limiting applicability and quality. We propose a novel text-to-image method that addresses these gaps by (i) enabling a simple control mechanism complementary to text in the form of a scene, (ii) introducing elements that substantially improve the tokenization process by employing domain-specific knowledge over key image regions (faces and salient objects), and (iii) adapting classifier-free guidance for the transformer use case. Our model achieves state-of-the-art FID and human evaluation results, unlocking the ability to generate high fidelity images in a resolution of \(512\times 512\) pixels, significantly improving visual quality. Through scene controllability, we introduce several new capabilities: **(i)** Scene editing, **(ii)** text editing withcan be controlled with text, such as style or color, others such as structure, form, or arrangement can only be loosely described at best [46]. This lack of control conveys a notion of randomness and weak user-influence on the image content and context [34]. Controlling elements additional to text have been suggested by [69], yet their use is confined to restricted datasets such as fashion items or faces. An earlier work by [23] suggests coarse control in the form of bounding boxes resulting in low resolution images.

**(ii) Human perception**. While images are generated to match human perception and attention, the generation process does not include any relevant prior knowledge, resulting in little correlation between generation and human attention. A clear example of this gap can be observed in person and face generation, where a dissonance is present between the importance of face pixels from the human perspective and the loss applied over the whole image [28, 66]. This gap is relevant to animals and salient objects as well.

**(iii) Quality and resolution**. Although quality has gradually improved between consecutive methods, the previous state-of-the-art methods are still limited to an output image resolution of \(256\times 256\) pixels [45, 41]. Alternative approaches propose a super-resolution network which results in less favorable visual and### Image tokenization

Image generation models based on discrete representation [59, 45, 47, 12, 13] follow a two-stage training scheme. First, an image tokenizer is trained to extract a discrete image representation. In the second stage, a generative model generates the image in the discrete latent space. Inspired by Vector Quantization (VQ) techniques, VQ-VAE [59] learns to extract a discrete latent representation by performing on-line clustering. VQ-VAE-2 [47] presented a hierarchical ar

Figure 2: Qualitative comparison with previous work. The text and generated images for [67, 45, 41] were taken from [41]. For CogView [12] we use the released \(512\times 512\) model weights, applying self-reranking of \(60\) for post-generation selection.chitecture composed of VQ-VAE models operating at multiple scales, enabling faster generation compared with pixel space generation. The DALL-E [45] text-to-image model used dVAE, which uses gumbel-softmax [26, 39], relaxing the VQ-VAE's online clustering. Recently, VQGAN [13] added adversarial and perceptual losses [68] on top of the VQ-VAE reconstruction task, producing reconstructed images with higher quality. In our work, we modify the VQ-GAN framework by adding perceptual losses to specific image regions, such as faces and salient objects, which further improve the fidelity of the generated images.

### Image-to-image generation

Generating images from segmentation maps or scenes can be viewed as a conditional image synthesis task [71, 38, 24, 61, 62, 42]. Specifically, this form of image synthesis permits more controllability over the desired output. CycleGAN [71] trained a mapping function from one domain to the other. UNIT [38] projected two different domains into a shared latent space and used a per-domain decoder to re-synthesize images in the desired domain. Both methods do not require supervision between domains. pix2pix [24] utilized conditional GANs together with a supervised reconstruction loss. pix2pixHDtation in [13]. In our implementation the inputs and outputs of VQ-SEG are \(m\) channels, representing the number of classes for all semantic segmentation groups \(m=m_{p}+m_{h}+m_{f}+1\), where \(m_{p}\), \(m_{h}\), \(m_{f}\) are the number of categories for the panoptic segmentation [63], human segmentation [35], and face segmentation extracted with [5] respectively. The additional channel is a map of the edges separating the different classes and instances. The edge channel provides both separations for adjacent instances of the same class, and emphasis on scarce classes with high importance, as edges (perimeter) are less biased towards larger categories than pixels (area).

### Adhering to human emphasis in the token space

We observe an inherent upper-bound on image quality when generating images with the transformer, stem

Figure 4: Generating images through edited scenes. For an input text (a) and the segmentations extracted from an input image (b), we can re-generate the image (c) or edit the segmentations (d) by replacing classes (top) or adding classes (bottom), generating images with newming from the tokenization reconstruction method. In other words, quality limitations of the VQ image reconstruction method inherently transfer to quality limitations on images generated by the transformer. To that end, we introduce several modifications to both the segmentation and image reconstruction methods. These modifications are losses in the form of emphasis (specific region awareness) and perceptual knowledge (feature-matching over task-specific pre-trained networks).

### Face-aware vector quantization

While using a scene as an additional form of conditioning provides an implicit prior for human preference, we institute explicit emphasis in the form of additional losses, explicitly targeted at specific image regions.

We employ a feature-matching loss over the activations of a pre-trained face-embedding network, introducing "awareness" of face regions and additional perceptual information, motivating high-quality face reconstruction.

Before training the face-aware VQ (denoted as \(\mathrm{VQ}\)-\(\mathrm{IMG}\)), faces are located using the semantic segmentation information extracted for VQ-SEG. The face locations are then used during the face-aware VQ training stage, running up to \(k_{f}\) faces per image from the ground-truth and reconstructed images through the face-embedding network. The face loss can then be formulated as following:

\[\mathcal{L}_{\text{Face}weighted binary cross-entropy loss added to the conditional VQ-VAE losses defined by [13].

### Object-aware vector quantization

We generalized and extend the face-aware VQ method to increase awareness and perceptual knowledge of objects defined as "things" in the panoptic segmentation categories. Rather than a specialized face-embedding network, we employ a pre-trained VGG [52] network trained on ImageNet [33], and introduce a feature-matching loss representing the perceptual differences between the object crops of the reconstructed and ground-truth images. By running the feature-matching over image crops, we are able to increase the output image resolution from \(256\times 256\) by simply adding to VQ-IMG an additional down-sample and up-sample layer to the encoder and decoder respectively. Similarly to Eq. 1, the loss can be formulated as:

\[\mathcal{L}_{\text{Obj}}=\sum_{k}\sum_{l}\alpha_{o}^{l}\|\operatorname{ VGG}^{l}(\hat{c}_{o}^{k})-\operatorname{VGG}^{l}(c_{o}^{k})\|,\] (3)

where \(\Experiments were performed with a 4 billion parameter transformer, generating a sequence of \(256\) text tokens, \(256\) scene tokens, and \(1024\) image tokens, that are then decoded into an image with a resolution of \(256\times 256\) or \(512\times 512\) pixels (depending on the model of choice).

### Datasets

The scene-based transformer is trained on a union of CC12m [7], CC [51], and subsets of YFCC100m [55] and Redcaps [10], amounting to \(35\)m text-image pairs. MS-COCO [37] is used unless otherwise specified. VQ-SEG and VQ-IMG are trained on CC12m, CC, and MS-COCO.

### Metrics

The goal of text-to-image generation is to generate high-quality and text-aligned images from a human perspective. Different metrics have been suggested to mimic the human perspective, where some are considered more reliable than others. We consider human evaluation the highest authority when evaluating image quality and text-alignment, and rely on FID [19] to increase evaluation confidence and handle cases### Scene editing and anchoring

Rather than editing certain regions of images as demonstrated by [45], we introduce new capabilities of generating images from existing or edited scenes. In Fig. 4, two scenarios are considered. In both scenarios the semantic segmentation is extracted from an input image, and used to re-generate an image conditioned on the input text. In the top row, the scene is edited, replacing the 'sky' and 'tree' categories with 'sea', and the 'grass' category with 'sand', resulting in a generated image adhering to the new scene. A simple sketch of a giant dog is added to the scene in the bottom row, resulting in a generated image corresponding to the new scene without any change in text.

Fig. 5 demonstrates the ability to generate new interpretations of existing images and scenes. After extracting the semantic segmentation from a given image, we re-generate the image conditioned on the input scene and edited text.



### Storytelling through controllability

To demonstrate the applicability of harnessing scene control for story illustrations, we wrote a children story, and illustrated it using our method. The main advantages of using simple sketches as additional inputs in this case, are (i) that authors can translate their ideas into paintings or realistic images, while being less susceptible to the "randomness" of text-to-image generation, and (ii) improved consistency## References

* [1] Speakers Give Sound Advice. Syracuse post standard. _March_, 28:18, 1911.
* [2] Shane Barratt and Rishi Sharma. A note on the inception score. _arXiv preprint arXiv:1801.01973_, 2018.
* [3] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural image synthesis. _arXiv preprint arXiv:1809.11096_, 2018.
* [4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* [5] Adrian Bulat and Georgios Tzimiropoulos. How far are we from solving the 2d & 3d face alignment problem? (and a dataset of 230,000* [31] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* [32] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. _arXiv preprint arXiv:1312.6114_, 2013.
* [33] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. _Advances in neural information processing systems_, 25, 2012.
* [34] Bowen Li, Xiaojuan Qi, Thomas Lukasiewicz, and Philip Torr. Controllable text-to-image generation. _Advances in Neural Information Processing Systems_, 32, 2019.
* [35] Peike Li, Yunqi Xu, Yunchao Wei, and Yi Yang. Self-correction for human parsing. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2020.
* [36] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollar. Focal loss for dense object detection. In _* [60] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* [61] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. Video-to-video synthesis. _arXiv preprint arXiv:1808.06601_, 2018.
* [62] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. High-resolution image synthesis and semantic manipulation with conditional gans. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 8798-8807, 2018.
* [63] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick. Detectron2. https://github.com/facebookresearch/detectron2, 2019.
*A Additional implementation details

### Vq-Seg

VQ-SEG is trained for \(600k\) iterations, with a batch size of \(48\), dictionary size of \(1024\). The number of segmentation categories per-group are \(m_{p}=133\) for the panoptic segmentation, \(m_{h}=20\) for the human parsing, and \(m_{f}=5\) for the face parsing. The per-category weight function follows the notation:

\[\alpha_{cat}=\begin{cases}20,&\text{if cat}\in[154,...,158]\\ 1,&\text{otherwise},\end{cases}\] (4)

where cat \(\in[154,...,158]\) are the face-parts categories eye-brows, eyes, nose, outer-mouth, and inner-mouth.

### Vq-Img

\(\mathrm{VQ}\)-\(\mathrm{IMG}_{256}\) and \(\mathrm{VQ}\)-\(\mathrm{IMG}_{Figure 7: Additional samples generated from challenging text inputs.Figure 8: Additional samples generated from challenging text inputs.Figure 10: Additional samples generated (b) from text and segmentation inputs (a).

Figure 9: Additional samples generated (b) from text and segmentation inputs (a).Figure 11: Additional samples generated (b) from text and segmentation inputs (a).

Figure 12: Additional samples generated (b) from text and segmentation inputs (a).